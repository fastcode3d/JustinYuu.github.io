---
layout: post
title: "Daily Paper 42: Meta-Tranfer Learning for Few-Shot Learning"
description: "Notes"
categories: [CV-Meta]
tags: [Paper]
redirect_from:
  - /2020/01/06/
---

# Daily Paper 42 - Meta-Tranfer Learning for Few-Shot Learning  

## Introduction  

nndl这门课需要复现一篇论文，我选了这篇作为复现的目标论文，一是因为小样本学习是我们小组目前拟研究的方向，二是因为小样本学习需要的训练集很小，训练起来比较方便，会比较省时间，三是因为作者提供了源码出来，最后如果写不完也可以直接参考作者代码。  

这篇文章是新国大、天津大学和马普所的四位学者共同发表在CVPR2019上的，主要研究小样本学习中的元学习方法的迁移学习。目前大家都知道，小样本学习这一方向很火，而小样本学习中元学习(meta-learning)又是非常好用的一种方法，所谓的元学习，主要的目的是让搭建的神经网络自己学会如何学习，从而能够用比较小的样本来完成比较难的学习任务。元学习的中心思想是利用大量的相似的小样本任务来学习到对于一些只有少量标注样本的新任务的通用学习方式，与当今深度学习网络利用大量的数据来进行学习不同，元学习的数据规模一般较小，如果仍然采用一些深层网络架构很容易过拟合，因此元学习一般采用浅层神经网络来实现，浅层神经网络+少量的训练数据，导致元学习的训练量一般是比较少的，那么训练的结果也就没有深层的网络训练出的效果好，学习的信息也相对较少。因此作者就想解决元学习无法应用在深层神经网络这一现象，在这篇文章中，作者提出了一个新的小样本学习方法，称作元迁移学习(meta-transfer learning, MTL)，该方法最大的特点，就是能够把深层的神经网络应用在小样本学习任务中。具体而言，MTL中的meta，意味着训练多个任务，而transfer是通过学习每一个任务中DNN权重的缩放和偏移函数来实现的。此外，作者还提出了hard task(HT) meta-batch方案，作为MTL的一种有效的学习课程。作者的模型在miniImageNet和FewShot-CIFAR100两个小样本学习的benchmarks上的5class1shot和5class5shot识别任务中实现了STOA。作者还进行了隔离实验，表明无论是MTL还是HT meta-batch都有助于快速的训练收敛和高准确率。  

对于小样本学习而言，一般可以分为两种类别：数据增强与基于任务的元学习，数据增强其实算是一种曲线救国的策略，虽然提供的数据集很小，但是通过多种多样的数据增强的方式，可以将原本数量很小的数据集扩充多倍，也可以将多任务的数据合在一起进行训练，不过想一想就知道，增强的数据总归不是新的数据，很难带来新的可学习的信息，而简单的数据整合也会导致不同任务的数据集方差太大，不利于统一化的学习。那么第一种方法效果不好的情况下，大家就普遍的将视角聚焦在了第二种，也就是元学习上。正如之前提到的，元学习是一种基于任务的学习方法，它的主要目标是从学习多个任务中积累经验，其base-learning主要学习的是对每一个单独的任务的数据分布进行建模。当前的一个STOA方法Model-Agnostic Meta-Learning(MAML)就是在学习寻找一个最佳的初始化状态，从而能够快速的将base-learner适应一个新任务，这个方法具有任务不可知性，这就使得该算法可以泛化到小样本学习和无监督强化学习中。不过在作者看来，该类型的方法有两大限制，第一是这些方法经常需要一大堆相似的任务来进行元训练，这本身就违背了小样本学习的初衷；第二是每一个任务通常都会由一个低复杂度的base-learner（比如浅层神经网络）建模，无法使用一些功能更为强大的深层网络，比如resnet等等，这也就是一开始作者所想要解决的最大痛点。  

那么简要的总结一下，这篇paper主要提出了一个新的元学习方法，叫做meta-transfer learning(MTL)，利用了迁移学习和元学习的优点，使用深度网络作为base-learner，并能够使深度网络在样本很少的情况下迅速收敛，并减少了过拟合的可能性。该方法中的transfer意味着在大规模的数据上训练的DNN权重能够仅通过两个操作：Scaling and Shifting(SS)来完成到其他任务的迁移；而meta意外着这些操作的参数全部可以作为超参数来看待，在小样本数据集上进行训练。在整个过程中，训练的DNN的权重是未改变的，这就避免了所谓的“catastrophic forgetting”问题，即在适应一个新的任务的过程中忘记了之前学习的广义状态。该paper的第二个主要的贡献是提出了一个有效的元学习训练的课程，课程学习和难例挖掘都说明了一个更好的训练数据的排序可以得到更快的收敛和更强大的表现，所以作者设计了一个HT meta-batch的训练策略，提供了一个有挑战性但是又很有效的学习策略。作者用一张图来介绍了他们的方法和其他方法的不同：迁移学习是最简单的，用一个比较大的网络和比较多的数据来进行训练，然后进行微调；元学习其次，用N个模型学习N个任务，然后用第N+1个模型来训练第N+1个任务；而Meta-Batch是将这N个任务分成多个batch，在每一个batch内部都有k个任务来训练；作者的MTL模型和之前的都不同，他们是用一个DNN来训练一个模型，然后对于N个不同的任务，都通过在这一个模型上进行SS和微调来解决，而在第N+1个任务上，在该模型上用之前训练得到的SS方式和针对该任务独特的微调来进行解决；至于作者的HT Meta-Batch，则是Meta-Batch的改进版本，在每一个batch内部使用k个普通任务和k'个困难任务，在一个batch内部进行在线的重采样。  

## Related Works  



---
本博客支持disqus实时评论功能，如有错误或者建议，欢迎在下方评论区提出，共同探讨。  
